{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "from modules.classifer_dataset import ClassifierDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset.\n",
    "titanic_train_csv_df = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_test_csv_df = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\")\n",
    "\n",
    "print(f'titanic_train_csv_df shape: {titanic_train_csv_df.shape}')\n",
    "print(f'titanic_test_csv_df shape: {titanic_test_csv_df.shape}')\n",
    "\n",
    "# concat test and train to get metadata for non numeric categories\n",
    "csv_union_df = pd.concat([titanic_test_csv_df, titanic_train_csv_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_column_name = \"survived\"\n",
    "[train_encoded_df, test_encoded_df] = ClassifierDataset.onehot_encode_datafames([titanic_train_csv_df, titanic_test_csv_df])\n",
    "\n",
    "\n",
    "train_ds = ClassifierDataset(train_encoded_df, label_column_name)\n",
    "test_ds = ClassifierDataset(test_encoded_df, label_column_name)\n",
    "\n",
    "batch_size = int(len(train_ds) / 10)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, shuffle=True)\n",
    "print(f'{len(train_ds)} training records in with batch size {batch_size}, {len(test_ds)} records for test')\n",
    "\n",
    "print(f'train has {train_ds.get_feature_count()} features')\n",
    "print(f'test has {test_ds.get_feature_count()} features')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_RATE_01 = .20\n",
    "\n",
    "# TODO have a config for layers and wire them from it\n",
    "\n",
    "class TitanicSurvivalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_feature_columns):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(num_feature_columns, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=DROPOUT_RATE_01), \n",
    "            nn.Linear(64, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=DROPOUT_RATE_01), \n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=DROPOUT_RATE_01), \n",
    "            nn.Linear(32, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "first_training_record, _ = train_ds[0]\n",
    "num_features = first_training_record.shape[-1]\n",
    "model = TitanicSurvivalNeuralNetwork( num_features)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "num_features = train_ds.get_feature_count()\n",
    "model = TitanicSurvivalNeuralNetwork( num_features )\n",
    "model.to(device)\n",
    "\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_correct_count, epoch_pred_count = 0, 0\n",
    "    for X, y in train_dataloader:\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.reshape(-1, 1).to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        y_pred_guess = torch.round(y_pred)\n",
    "        batch_num_correct = (y == y_pred_guess).sum()\n",
    "        epoch_correct_count += batch_num_correct\n",
    "        epoch_pred_count += len(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], {epoch_correct_count} of {epoch_pred_count} correct {(100*epoch_correct_count/epoch_pred_count):.1f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb671ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define your loss function (e.g., Binary Cross-Entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Commonly used for binary classification\n",
    "\n",
    "# Lists to store results\n",
    "test_losses = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test batches\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    for inputs, labels in test_dataloader:\n",
    "        # just make this an array of the labels (instead array of arrays with one element)\n",
    "        labels = labels.reshape(-1)\n",
    "\n",
    "        y_pred = model(inputs)\n",
    "        y_pred = y_pred.reshape(labels.shape)  # make sure it matches\n",
    "\n",
    "        loss = criterion(y_pred, labels)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "        y_pred_guess = torch.round(y_pred)\n",
    "        \n",
    "        all_preds.extend(y_pred_guess.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate overall metrics\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
