{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# good references\n",
    "# https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
    "# https://www.tensorflow.org/tutorials/load_data/csv\n",
    "\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO there is a numpy util to get files from within tgz's and zips\n",
    "##!curl -s 'https://storage.googleapis.com/0x19f.com/media/kaggle-mlb-pitch-data-2015-2018.tgz' | tar xz\n",
    "\n",
    "BASEBALL_FILES_BASE = os.getcwd() + \"/kaggle-mlb-pitch-data-2015-2018/\"\n",
    "BASEBALL_FILES_BASE = BASEBALL_FILES_BASE + \"2019_\"  # just look at 2019 to speed things up\n",
    "\n",
    "pitches = pd.read_csv(BASEBALL_FILES_BASE + 'pitches.csv')\n",
    "atbats = pd.read_csv(BASEBALL_FILES_BASE + 'atbats.csv')\n",
    "\n",
    "df = pd.merge(pitches, atbats, how='inner', on='ab_id')\n",
    "\n",
    "df = df[[\n",
    "    # situation stuff\n",
    "    \"ab_id\", \"inning\", \"top\", \"outs\", \"on_1b\", \"on_2b\", \"on_3b\",\n",
    "\n",
    "    # matchup\n",
    "    \"pitcher_id\", \"p_throws\", \"batter_id\", \"stand\",\n",
    "\n",
    "    # specific pitch state\n",
    "    \"pitch_num\", \"b_count\", \"s_count\",\n",
    "    # TODO need to add running total of pitches for the current pitcher, and maybe strike percentage\n",
    "    # TODO add the previous n pitches before and their code outcome\n",
    "        \n",
    "    # this is the label we're trying to predict\n",
    "    \"code\",\n",
    "    \"pitch_type\",\n",
    "    # TODO also make the label have the location. not just FF (fourseam fastball, but also high inside)\n",
    "]]\n",
    "\n",
    "# get appropriate types. \n",
    "# TODO prob can use a schema on the CSV read to get only the columns we're after, alias them to be more\n",
    "#      useful, and cast to the right type\n",
    "# TODO consider to_ordinal for cumulative things maybe? inning, outs, pitch_num, b_count, s_count\n",
    "\n",
    "# TODO there is a numpy util to get files from within tgz's and zips\n",
    "##!curl -s 'https://storage.googleapis.com/0x19f.com/media/kaggle-mlb-pitch-data-2015-2018.tgz' | tar xz\n",
    "\n",
    "BASEBALL_FILES_BASE = os.getcwd() + \"/kaggle-mlb-pitch-data-2015-2018/\"\n",
    "BASEBALL_FILES_BASE = BASEBALL_FILES_BASE + \"2019_\"  # just look at 2019 to speed things up\n",
    "\n",
    "\n",
    "pitches = pd.read_csv(BASEBALL_FILES_BASE + 'pitches.csv')\n",
    "atbats = pd.read_csv(BASEBALL_FILES_BASE + 'atbats.csv')\n",
    "\n",
    "df = pd.merge(pitches, atbats, how='inner', on='ab_id')\n",
    "\n",
    "df = df[[\n",
    "    # situation stuff\n",
    "    \"ab_id\", \"inning\", \"top\",  \n",
    "    \n",
    "    # would get baserunner stuff but these were all zero.\n",
    "    #\"on_1b\", \"on_2b\", \"on_3b\",\n",
    "\n",
    "    # matchup\n",
    "    \"pitcher_id\", \"p_throws\", \"batter_id\", \"stand\",\n",
    "\n",
    "    # TODO add catcher, team\n",
    "    # TODO need to add running total of pitches for the current pitcher, and maybe strike percentage\n",
    "    # TODO add the previous n pitches before and their code outcome\n",
    "    \n",
    "    # specific pitch state, this becomes situation and is then dropped\n",
    "    \"pitch_num\", \"b_count\", \"s_count\", \"outs\",      \n",
    "    \n",
    "    # this is the label we're trying to predict\n",
    "    # TODO also make the label have the location, not just FF (but also high inside)\n",
    "    \"pitch_type\",\n",
    "]]\n",
    "\n",
    "# remove anything without values, hard to have defaults\n",
    "df = df.dropna()\n",
    "\n",
    "# set the appropriate types. \n",
    "TYPE_DICT = {\n",
    "    # string features\n",
    "    'p_throws': 'string',\n",
    "    'stand': 'string',\n",
    "\n",
    "    # small card numeric features and player ids: treat as strings to avoid norming with mean\n",
    "    'top': 'string',\n",
    "    'inning': 'string',\n",
    "    'pitch_num': 'string', \n",
    "    'pitcher_id': 'string', \n",
    "    'batter_id': 'string',\n",
    "}\n",
    "df = df.astype(TYPE_DICT)\n",
    "\n",
    "\n",
    "# add the pitch count sitaution, concat as an int8 array to loose the decimals,\n",
    "# and then convert to a string so we can one_shot encode it too. \n",
    "df[\"situation\"] = df[[\"b_count\", \"s_count\", \"outs\"]].astype(\"int8\").to_numpy().tolist()\n",
    "df['situation'] = df['situation'].astype(\"string\")\n",
    "\n",
    "\n",
    "# finally, remove pitch_type and ab_id and stuff already in \"situation\" \n",
    "# because we dont need them at this point\n",
    "df = df.drop(['ab_id', 'b_count', 's_count', \"outs\"], axis=1)\n",
    "\n",
    "\n",
    "# TODO remove this truncation when solid\n",
    "df = df.truncate(after=150*1000)\n",
    "\n",
    "# ok, ready to split into features and labels. pitch_features is a copy\n",
    "# of the df source, but we drop the label feature.\n",
    "pitch_features = df.copy()\n",
    "pitch_features.pop(\"pitch_type\")\n",
    "\n",
    "# the labels we process using a lookup to normalize to one_hot encoding   \n",
    "all_pitch_types = np.unique(df['pitch_type'].unique())\n",
    "pitch_type_lookup = layers.StringLookup(vocabulary=all_pitch_types, output_mode='one_hot')\n",
    "pitch_labels = pitch_type_lookup(df.pitch_type)\n",
    "\n",
    "print(pitch_features.info())\n",
    "print(pitch_features.shape)\n",
    "\n",
    "pitch_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO refactor this into a library, it is also in titanic.ipynb\n",
    "\n",
    "# attempts to (eventually) preprocess a dataframe of features\n",
    "# returns all of the collected inputs and preprocessed features \n",
    "def process_dataframe_inputs(feature_df):\n",
    "\n",
    "  # first we walk through each feature in the dataframe, make an input for it,\n",
    "  # normalize it using the feature_df data, then collect it into lists of\n",
    "  # inputs and preprocessed inputs, respectively\n",
    "\n",
    "  input_layers, preprocessed_inputs  = [], []\n",
    "  for name, col in feature_df.items():\n",
    "\n",
    "    # TODO do this with log()\n",
    "    print(\"processing %s ...\" % name)\n",
    "\n",
    "    # convert to tensorflow types. this is straightforward except \n",
    "    # for objects, treat them as strings.\n",
    "    dtype_str = str(col.dtype) if col.dtype != object else \"string\"\n",
    "    cur_dtype = tf.as_dtype(dtype_str)\n",
    "    cur_input = tf.keras.Input(shape=(1,), name=name, dtype=cur_dtype)\n",
    "    input_layers.append(cur_input)  \n",
    "\n",
    "    if cur_dtype == tf.string:\n",
    "      lookup = layers.StringLookup(vocabulary=np.unique(col))    \n",
    "      norm = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "      normalized_input = norm(lookup(cur_input))\n",
    "      preprocessed_inputs.append(normalized_input)\n",
    "\n",
    "    else:\n",
    "      norm = layers.Normalization(axis=None)\n",
    "      norm.adapt(col)\n",
    "      normalized_input = norm(cur_input)\n",
    "      preprocessed_inputs.append( normalized_input )\n",
    "\n",
    "  # prepare the return values by concatentating the preprocessed inputs\n",
    "  # and creating a model for which they serve as outputs.\n",
    "  # \n",
    "  # returns the input layers and the processed_inputs, respectively\n",
    "\n",
    "  preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "  preprocessing_model = tf.keras.Model(input_layers, preprocessed_inputs_cat)\n",
    "  preprocessed_inputs = preprocessing_model(input_layers)\n",
    "\n",
    "  return input_layers, preprocessed_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers, preprocessed_inputs = process_dataframe_inputs(pitch_features)\n",
    "\n",
    "seqential_hidden_model = tf.keras.Sequential([\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(512, activation=\"relu\"),\n",
    "  layers.Dense(256, activation=\"relu\"),\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(16, activation=\"relu\"),\n",
    "  layers.Dense(len(pitch_type_lookup.get_vocabulary()), activation=\"softmax\"),\n",
    "])\n",
    "result = seqential_hidden_model(preprocessed_inputs)\n",
    "pitch_model = tf.keras.Model(input_layers, result)\n",
    "pitch_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pitch_features_dict = {\n",
    "  name: np.array(value) for name, value in pitch_features.items()\n",
    "}\n",
    "# TODO add validation_split=0.33, \n",
    "hist = pitch_model.fit(x=pitch_features_dict, y=pitch_labels, epochs=200, batch_size=10*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, curX = plt.subplots()\n",
    "curX.set_xlabel(\"epoch\")\n",
    "vals = ['loss', 'accuracy'] \n",
    "for (i, val) in enumerate(vals):\n",
    "    color = list(mcolors.BASE_COLORS.values())[i]\n",
    "    curX.plot(hist.epoch, hist.history[val], color=color)\n",
    "    curX.set_ylabel(val, color=color)\n",
    "    if val != vals[-1]:\n",
    "        curX = curX.twinx() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
