{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q numpy pandas tensorflow\n",
    "\n",
    "## references\n",
    "# https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "# https://www.kaggle.com/datasets/pschale/mlb-pitch-data-20152018/code\n",
    "# https://www.kaggle.com/code/ryancmcv/mlb-pitch-data\n",
    "# https://stackoverflow.com/questions/64689483/how-to-do-multiclass-classification-with-keras\n",
    "\n",
    "# to_ordinal https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_ordinal\n",
    "# to_categorical https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
    "\n",
    "# TODO filter features that fewer than n instances (ie pitchers who only pitched a game or so.) n=5 maybe?\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Normalization\n",
    "from keras.layers import IntegerLookup\n",
    "from keras.layers import StringLookup\n",
    "\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO there is a numpy util to get files from within tgz's and zips\n",
    "##!curl -s 'https://storage.googleapis.com/0x19f.com/media/kaggle-mlb-pitch-data-2015-2018.tgz' | tar xz\n",
    "\n",
    "BASEBALL_FILES_BASE = os.getcwd() + \"/kaggle-mlb-pitch-data-2015-2018/\"\n",
    "BASEBALL_FILES_BASE = BASEBALL_FILES_BASE + \"2019_\"  # just look at 2019 to speed things up\n",
    "\n",
    "\n",
    "pitches = pd.read_csv(BASEBALL_FILES_BASE + 'pitches.csv')\n",
    "atbats = pd.read_csv(BASEBALL_FILES_BASE + 'atbats.csv')\n",
    "\n",
    "df = pd.merge(pitches, atbats, how='inner', on='ab_id')\n",
    "\n",
    "df = df[[\n",
    "    # situation stuff\n",
    "    \"ab_id\", \"inning\", \"top\", \"outs\", \"on_1b\", \"on_2b\", \"on_3b\",\n",
    "\n",
    "    # matchup\n",
    "    \"pitcher_id\", \"p_throws\", \"batter_id\", \"stand\",\n",
    "\n",
    "    # specific pitch state\n",
    "    \"pitch_num\", \"b_count\", \"s_count\",\n",
    "    # TODO need to add running total of pitches for the current pitcher, and maybe strike percentage\n",
    "    # TODO add the previous n pitches before and their code outcome\n",
    "    # \n",
    "    \n",
    "    # this is the label we're trying to predict\n",
    "    \"code\",\n",
    "    \"pitch_type\",\n",
    "    # TODO also make the label have the location. not just FF (fourseam fastball, but also high inside)\n",
    "]]\n",
    "\n",
    "# remove anything without values, hard to have defaults\n",
    "# df = df.dropna(subset=['pitch_type', 'code'])\n",
    "df = df.dropna()\n",
    "\n",
    "## this is our current target label TODO make this categorical \n",
    "df[\"is_fastball\"] = np.where(df[\"pitch_type\"].isin([\"FC\",\"FF\",\"FT\",]), 1, 0)\n",
    "\n",
    "# get appropriate types. \n",
    "# TODO prob can use a schema on the CSV read to get only the columns we're after, alias them to be more\n",
    "#      useful, and cast to the right type\n",
    "# TODO consider to_ordinal for cumulative things maybe? inning, outs, pitch_num, b_count, s_count\n",
    "\n",
    "\n",
    "# TODO revisit the sizes here if it helps\n",
    "LARGE_INT = \"int64\"\n",
    "SMALL_INT = LARGE_INT\n",
    "\n",
    "df = df.astype({\n",
    "    'ab_id': LARGE_INT,\n",
    "    'inning': SMALL_INT,\n",
    "    'top': SMALL_INT,\n",
    "    'outs': SMALL_INT,\n",
    "    'on_1b': SMALL_INT,\n",
    "    'on_2b': SMALL_INT,\n",
    "    'on_3b': SMALL_INT,\n",
    "    'pitch_num': LARGE_INT,\n",
    "    'b_count': SMALL_INT,\n",
    "    's_count': SMALL_INT,\n",
    "    'p_throws': 'string',\n",
    "    'stand': 'string',\n",
    "  })\n",
    "\n",
    "\n",
    "# drop ab_id now that join has occured, not a useful feature\n",
    "df = df.drop(['ab_id', \"code\",\"pitch_type\"], axis=1)\n",
    "\n",
    "# TODO remove this truncation when solid\n",
    "df = df.truncate(after=1000)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_dataframe = df.sample(frac=0.2, random_state=1337)\n",
    "train_dataframe = df.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(val_dataframe))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    copied_dataframe = dataframe.copy()\n",
    "    labels = copied_dataframe.pop(\"is_fastball\")\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(copied_dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(copied_dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_feature_with_normalizer(feature, name, dataset, normalizer):\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "    \n",
    "    normalizer.adapt(feature_ds)\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "feature_types = {\n",
    "    # TODO these become int32, we should maybe treat them that way the whole way through\n",
    "    # TODO better ... use the pd type here too \n",
    "    SMALL_INT : {\n",
    "        \"features\" : [\n",
    "            \"inning\", \"top\", \"outs\", \"on_1b\", \"on_2b\", \"on_3b\", \"b_count\", \"s_count\", \n",
    "            \"pitcher_id\", \"batter_id\", \"pitch_num\", \n",
    "        ],\n",
    "        \"normalizer_provider\" : lambda : IntegerLookup(output_mode=\"binary\")\n",
    "    },\n",
    "    \"string\" : {\n",
    "        \"features\" : [\"p_throws\", \"stand\",],\n",
    "        \"normalizer_provider\" : lambda : StringLookup(output_mode=\"binary\")\n",
    "    },\n",
    "}\n",
    "\n",
    "feature_inputs_combined, feature_layers_combined = [], []\n",
    "\n",
    "for feature_type, feature_info in feature_types.items():\n",
    "    for feature_name in feature_info[\"features\"]:\n",
    "        feature_input = keras.Input(shape=(1,), name=feature_name, dtype=feature_type)\n",
    "        feature_inputs_combined.append(feature_input)\n",
    "\n",
    "        normalizer = feature_info[\"normalizer_provider\"]()\n",
    "        feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\n",
    "        feature_layers_combined.append(feature_encoded)\n",
    "        print(\"encoded: %s (%s)\" % (feature_name, feature_type))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO eval https://stackoverflow.com/questions/69933345/expected-min-ndim-2-found-ndim-1-full-shape-received-none\n",
    "\n",
    "    \n",
    "\n",
    "all_features = layers.concatenate(feature_layers_combined)\n",
    "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = keras.Model(feature_inputs_combined, output)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(train_ds, epochs=50, validation_data=val_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
