{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 18:58:35.167368: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-22 18:58:35.209105: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-22 18:58:35.210374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-22 18:58:36.156944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -q numpy pandas tensorflow\n",
    "\n",
    "## references\n",
    "# https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "# https://www.kaggle.com/datasets/pschale/mlb-pitch-data-20152018/code\n",
    "# https://www.kaggle.com/code/ryancmcv/mlb-pitch-data\n",
    "# https://stackoverflow.com/questions/64689483/how-to-do-multiclass-classification-with-keras\n",
    "\n",
    "# to_ordinal https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_ordinal\n",
    "# to_categorical https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
    "\n",
    "# TODO filter features that fewer than n instances (ie pitchers who only pitched a game or so.) n=5 maybe?\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 242 samples for training and 61 for validation\n"
     ]
    }
   ],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "val_dataframe = df.sample(frac=0.2, random_state=1337)\n",
    "train_dataframe = df.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(val_dataframe))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'age': <tf.Tensor: shape=(), dtype=int64, numpy=57>, 'sex': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'cp': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'trestbps': <tf.Tensor: shape=(), dtype=int64, numpy=130>, 'chol': <tf.Tensor: shape=(), dtype=int64, numpy=236>, 'fbs': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'restecg': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'thalach': <tf.Tensor: shape=(), dtype=int64, numpy=174>, 'exang': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'oldpeak': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'slope': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'ca': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'thal': <tf.Tensor: shape=(), dtype=string, numpy=b'2'>}\n",
      "Target: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    copied_dataframe = dataframe.copy()\n",
    "    labels = copied_dataframe.pop(\"target\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(copied_dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(copied_dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'age': <tf.Tensor: shape=(), dtype=int64, numpy=57>, 'sex': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'cp': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'trestbps': <tf.Tensor: shape=(), dtype=int64, numpy=140>, 'chol': <tf.Tensor: shape=(), dtype=int64, numpy=192>, 'fbs': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'restecg': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'thalach': <tf.Tensor: shape=(), dtype=int64, numpy=148>, 'exang': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'oldpeak': <tf.Tensor: shape=(), dtype=float64, numpy=0.4>, 'slope': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'ca': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'thal': <tf.Tensor: shape=(), dtype=string, numpy=b'fixed'>}\n",
      "Target: tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"target\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)\n",
    "\n",
    "\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "def encode_feature_with_normalizer(feature, name, dataset, normalizer):\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "    \n",
    "    normalizer.adapt(feature_ds)\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding int64: sex using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85e42ed250>\n",
      "encoding int64: cp using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85e4599730>\n",
      "encoding int64: fbs using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85ec618850>\n",
      "encoding int64: restecg using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85ccebb1f0>\n",
      "encoding int64: exang using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85cced5e20>\n",
      "encoding int64: ca using <keras.src.layers.preprocessing.integer_lookup.IntegerLookup object at 0x7c85cceeb730>\n",
      "encoding string: thal using <keras.src.layers.preprocessing.string_lookup.StringLookup object at 0x7c85ccef4910>\n",
      "encoding float32: age using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85cce8e1f0>\n",
      "encoding float32: trestbps using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85ccea1850>\n",
      "encoding float32: chol using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85cce58e80>\n",
      "encoding float32: thalach using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85cce30bb0>\n",
      "encoding float32: oldpeak using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85cce79a00>\n",
      "encoding float32: slope using <keras.src.layers.preprocessing.normalization.Normalization object at 0x7c85cce30a30>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer should be called on a list of at least 1 input. Received: input_shape=()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     27\u001b[0m         feature_layers_combined\u001b[39m.\u001b[39mappend(feature_encoded)\n\u001b[1;32m     29\u001b[0m \u001b[39m# for feature_name in [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"ca\",]:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#     feature_input = keras.Input(shape=(1,), name=feature_name, dtype=\"int64\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m#     all_inputs.append(feature_input)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m#     feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m#     all_feature_layers.append(feature_encoded)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m all_features \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mconcatenate(all_feature_layers)\n\u001b[1;32m     54\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m)(all_features)\n\u001b[1;32m     55\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDropout(\u001b[39m0.5\u001b[39m)(x)\n",
      "File \u001b[0;32m~/src/etc/aiml-inference-playground/.venv/lib/python3.9/site-packages/keras/src/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.layers.concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcatenate\u001b[39m(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[39m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m Concatenate(axis\u001b[39m=\u001b[39;49maxis, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)(inputs)\n",
      "File \u001b[0;32m~/src/etc/aiml-inference-playground/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/src/etc/aiml-inference-playground/.venv/lib/python3.9/site-packages/keras/src/layers/merging/concatenate.py:98\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m@tf_utils\u001b[39m\u001b[39m.\u001b[39mshape_type_conversion\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape):\n\u001b[1;32m     96\u001b[0m     \u001b[39m# Used purely for shape validation.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(input_shape[\u001b[39m0\u001b[39m], \u001b[39mtuple\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA `Concatenate` layer should be called on a list of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat least 1 input. Received: input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m input_shape):\n\u001b[1;32m    103\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer should be called on a list of at least 1 input. Received: input_shape=()"
     ]
    }
   ],
   "source": [
    "\n",
    "all_feature_layers, all_inputs = [], []\n",
    "\n",
    "feature_types = {\n",
    "    \"int64\" : {\n",
    "        \"features\" : [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"ca\",],\n",
    "        \"normalizer_provider\" : lambda : IntegerLookup(output_mode=\"binary\")\n",
    "    },\n",
    "    \"string\" : {\n",
    "        \"features\" : [\"thal\",],\n",
    "        \"normalizer_provider\" : lambda : StringLookup(output_mode=\"binary\")\n",
    "    },\n",
    "    \"float32\" : {\n",
    "        \"features\" : [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"slope\", ],\n",
    "        \"normalizer_provider\" : lambda : Normalization()\n",
    "    },\n",
    "}\n",
    "\n",
    "feature_inputs_combined, feature_layers_combined = [], []\n",
    "\n",
    "for feature_type, feature_info in feature_types.items():\n",
    "    for feature_name in feature_info[\"features\"]:\n",
    "        feature_input = keras.Input(shape=(1,), name=feature_name, dtype=feature_type)\n",
    "        feature_inputs_combined.append(feature_input)\n",
    "        normalizer = feature_info[\"normalizer_provider\"]()\n",
    "        print(\"encoding %s: %s using %s\" %(feature_type, feature_name, normalizer))        \n",
    "        feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\n",
    "        feature_layers_combined.append(feature_encoded)\n",
    "\n",
    "# for feature_name in [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"ca\",]:\n",
    "#     feature_input = keras.Input(shape=(1,), name=feature_name, dtype=\"int64\")\n",
    "#     all_inputs.append(feature_input)\n",
    "#     normalizer = IntegerLookup(output_mode=\"binary\")\n",
    "#     feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\n",
    "#     all_feature_layers.append(feature_encoded)\n",
    "\n",
    "# string_categorial_features = [\"thal\"]\n",
    "# for feature_name in string_categorial_features:\n",
    "#     feature_input = keras.Input(shape=(1,), name=feature_name, dtype=\"string\")\n",
    "#     all_inputs.append(feature_input)\n",
    "#     normalizer = StringLookup(output_mode=\"binary\")\n",
    "#     feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\n",
    "#     all_feature_layers.append(feature_encoded)\n",
    "\n",
    "# numeric_categorial_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"slope\", ]\n",
    "# for feature_name in numeric_categorial_features:\n",
    "#     feature_input = keras.Input(shape=(1,), name=feature_name, dtype=\"float32\")\n",
    "#     all_inputs.append(feature_input)\n",
    "#     normalizer = Normalization()\n",
    "#     feature_encoded = encode_feature_with_normalizer(feature_input, feature_name, train_ds, normalizer)\n",
    "#     all_feature_layers.append(feature_encoded)\n",
    "\n",
    "\n",
    "all_features = layers.concatenate(feature_layers_combined)\n",
    "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(all_inputs, output)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"age\": 55,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 1,\n",
    "    \"trestbps\": 145,\n",
    "    \"chol\": 210,\n",
    "    \"fbs\": 1,\n",
    "    \"restecg\": 2,\n",
    "    \"thalach\": 150,\n",
    "    \"exang\": 0,\n",
    "    \"oldpeak\": 2.3,\n",
    "    \"slope\": 3,\n",
    "    \"ca\": 0,\n",
    "    \"thal\": \"fixed\",\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = model.predict(input_dict)\n",
    "\n",
    "print(\n",
    "    \"This particular patient had a %.1f percent probability \"\n",
    "    \"of having a heart disease, as evaluated by our model.\" % (100 * predictions[0][0],)\n",
    ")\n",
    "\n",
    "predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
